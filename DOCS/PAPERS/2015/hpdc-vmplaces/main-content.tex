\section{Introduction}
\label{sec:intro}
%\AL[AL]{1 page (including the abstract)}

% Although a lot of progress has been made on Cloud Computing (CC)
% system management, \aka Infrastructure-as-a-Service
% toolkits~\cite{moreno:2012}, most of the popular
% solutions~\cite{cloudstack, opennebula, openstack} continue to rely on
%  elementary Virtual Machine (VM) placement
% policies that prevent them from maximizing the usage of CC resources
% while guaranteeing VM resource requirements as defined by Service
% Level Agreements (SLAs).
% %% THE TEXT BELOW COMES FROM ISPA'13
% %%%%%%%
% Typically, a batch scheduling approach is used: VMs (i)~are allocated
% according to user requests for resource reservations, and (ii)~are
% tied to the nodes where they were deployed until their
% destruction. Besides the fact that users often overestimate their
% resource requirements, such static policies are definitely not optimal
% for CC providers, since the effective resource requirements of each
% operated VM may significantly vary during its lifetime.
% %%%%%%%
%  Dynamic strategies such as consolidation, load balancing
% and other SLA-ensuring algorithms have been deeply investigated \cite
% {feller:ccgrid12}, \cite{Hermenier:2009:ECM:1508293.1508300},
% \cite{5715067}, \cite{quesnel:cpe2012}, \cite{5328077},
% \cite{5935254}. \MS{Be more assertive on the motivation: Adrien, which
%   was the article for justification?}

Even if more flexible and often more efficient approaches to the
Virtual Machine Placement Problem (VMPP) have been developed
%by the research community
, most of the popular Cloud Computing (CC) system
management \cite{cloudstack, opennebula, openstack}, \aka
Infrastructure-as-a-Service toolkits~\cite{moreno:2012}, continue to
rely on elementary Virtual Machine (VM) placement policies that
prevent them from maximizing the usage of CC resources while
guaranteeing VM resource requirements as defined by Service Level
Agreements (SLAs).
% %% THE TEXT BELOW COMES FROM ISPA'13
% %%%%%%%
Typically, a batch scheduling approach is used: VMs are allocated
according to user requests for resource reservations and tied to
the nodes where they were deployed until their destruction. Besides
the fact that users often overestimate their resource requirements,
such static policies are definitely not optimal for CC providers,
since the effective resource requirements of each operated VM may
significantly vary during its lifetime.

An important impediment to the adoption of more advanced strategies
such as consolidation, load balancing and other SLA-ensuring
algorithms that have been deeply investigated by the research community
\cite{feller:ccgrid12, Hermenier:2009:ECM:1508293.1508300, 5715067,
  quesnel:cpe2012, 5328077, 5935254} is related to the experimental
processes that have been used to validate them: most VMPP proposals have
been evaluated either by leveraging ad-hoc simulators or small
testbeds. These evaluation environments are not accurate and not
representative enough to (i) ensure their correctness on real
platforms and (ii) perform fair comparisons between them.

% %
% \begin{figure}[ht]
% \vspace*{-.2cm}
% \begin{center}
%         \subcapcentertrue
%         \subfigure[Scheduling steps]{
%         \includegraphics[width=.45\linewidth]{figures/scheduling_steps.pdf}
%         \label{fig:scheduling_steps}}
%         \subfigure[Workload fluctuations during scheduling]{
%         \includegraphics[width=.45\linewidth]{figures/workload_fluctuations2.pdf}
%         \label{fig:workload_fluctuations}}
% \vspace*{-.2cm}
% \caption{VM scheduling in a master/worker architecture}
% \end{center}
% \label{fig:scheduling}
% \vspace*{-.2cm}
% \end{figure}
% %
% \MS[AL]{Must we keep the fig.: the arguments of complexity and time
%   requirements are already made in the remainder of the intro. If we
%   keep it, the discussion should be simplified.}
% Each VMPP mechanism is a complex system that can face
% important side-effects during each of its stages: Monitoring the
% resources usages, computing the schedule and applying the
% reconfiguration (see Figure \ref{fig:scheduling_steps}).
% %
% %
% As an example, a single master architecture can lead, \textit{a priori} to
% important drawbacks. First, during the computation and the application
% of a schedule, a single master cannot take into account new VM
% requirement violations. Second, the time needed to apply a new
% schedule can be particularly important: The longer the reconfiguration
% process, the higher the risk that the schedule may be outdated, due to
% the workload fluctuations, when it is eventually applied (see Figure
% \ref{fig:workload_fluctuations}). Finally, a single master node can
% lead to well-known fault-tolerance issues: A group of VMs may be
% temporarily isolated from the master node in case of a network
% disconnection or if the master node crashes.
% %
Implementing each proposal and evaluating it on representative
testbeds in terms of scalability, reliability and varying workload
changes would definitely be the most rigorous way to observe and
propose appropriate solutions for CC production infrastructures.
% and compare  with existing proposals.
However, \textit{in-vivo} (\ie real-world) experiments, if they can be
executed at all, are always expensive and tedious to perform (for
recent reference see~\cite{barker:pitfalls}). They may
even be counterproductive if the observed behaviors are clearly
different from the expected ones.

In this article, we propose \vmps, a dedicated simulation framework to
perform in-depth investigations of VM placement algorithms and compare
them in a fair way. To cope with real conditions such as the
increasing scale of modern data centers and the dynamicity of the
workloads that are specific to the CC paradigm, notably its elasticity
capacity, \vmps allows users to study large-scale scenarios that take
into account server crashes and that involve tens of thousands of VMs,
each of which executes a specific workload that evolves during the
simulation lifetime.
\AL[MS]{each of which executing?}

Built on top of the \sg toolkit~\cite{casanova:hal-01017319}, \vmps
provides three additional simulation facilities: a more abstract
representation of hosts and virtual machines, and two frameworks, one
for the management of load injection abstract and another one for the
extraction and analysis of execution traces. We believe that such a
tool will be beneficial to a large number of researchers in the field
of CC as it enables them to quickly validate
% AL->MS the characterisctics -> the trends,  simulations are
% well-known scientific intrusments to validate trends (and only trends).
the trends of a new proposal and compare it with existing
ones. This way, our approach allows \textit{in vivo} experiments to be
restricted to VMPP mechanisms that have the potential to handle CC
production infrastructures.

%
We chose to base \vmps on \sg since (i) the latter's relevance in
terms of performance and validity has already been
demonstrated~\cite{simgridpub} and (ii) because it has been recently
extended to integrate virtual machine abstractions and a live
migration model \cite{Hirofuchi:2013:ALM:2568486.2568524}.

To illustrate the relevance of \vmps, we have implemented the
essential mechanisms of three well-known VMPP approaches:
Entropy~\cite{Hermenier:2009:ECM:1508293.1508300},
Snooze~\cite{feller:ccgrid12}, and DVMS~\cite{quesnel:cpe2012}.
Besides being well-known from the literature, we chose
these three systems as they are built on three different software
architecture approaches: Entropy relies on a centralized model, Snooze
on a hierarchical one and DVMS on a fully distributed one.

Once the accuracy of \vmps validated, we have also investigated the
characteristics of these three strategies by studying their
scalability, reliability and reactivity (\ie the time to solve
resource violations, \aka SLA violation) criterions through several
simulations. This study reveal:
\begin{itemize}
\item The importance of the duration of the reconfiguration phase (\ie
  the step where VMs are relocated throughout the infrastructure) in
  comparison to the computation one (\ie the step where the scheduler
  try to solve the VMPP, see Section \ref{sec:vmpp} for a complete
  definition of both steps).
\item The quasi-inexistant impact of failures on the reactivity for
  all systems, especially if we are considering a 6 month failure
  rate;
\item The pros and cons of partitionning Snooze in small or large
  groups;
\item The pros and cons of a reactive vs a periodic scheduling
  strategies.
\item The interest of leveraging our toolkit to identify limitations
  of proposals and study variants and possible improvements.
\end{itemize}

The rest of the article is
organized as follow. Section~\ref{sec:vmpp} highlights the importance
of the scalability, reliability and reactivity criterions for the VM
Placement Problem.
Section~\ref{sec:sg} gives an overview of the \sg
framework on which our proposal is built. \ref{sec:injector}
introduces \vmps and discusses its general functioning. The three
algorithms implemented as use-cases are presented in
Section~\ref{sec:vm-schedulers} and evaluated in
Section~\ref{sec:experiments}. Section~\ref{sec:related} and
Section~\ref{sec:conclusion} present, respectively, related work as
well as a conclusion and future work.

\section{The VM Placement Problem}
\label{sec:vmpp}

A VMPP can be summarized in three-steps (see Figure
\ref{fig:scheduling_steps}): monitoring the resources usages,
computing a new schedule each time is it needed and applying the
resulting reconfiguration plan (\ie performing VM migration and
suspend/resume operations to realize to the new placement solution).
% that are mandatory to solve resource violations while optimizing resource usages).

\begin{figure}[ht]
\vspace*{-.2cm}
\begin{center}
        \subcapcentertrue
        \subfigure[Scheduling steps]{
        \includegraphics[width=.45\linewidth]{figures/scheduling_steps.pdf}
        \label{fig:scheduling_steps}}
        \subfigure[Workload fluctuations during scheduling]{
        \includegraphics[width=.45\linewidth]{figures/workload_fluctuations2.pdf}
        \label{fig:workload_fluctuations}}
\vspace*{-.2cm}
\caption{VM scheduling Phases}
\end{center}
\label{fig:scheduling}
\vspace*{-.2cm}
\end{figure}
%

VMPP solutions stand and fall with their scalability, reliability and
reactivity of properties, because they have to maintain a placement
that satisfies the requirements of all VMs while optimizing the usage
of CC resources. For instance, a naive implementation of a
master/worker approach as described in
Figure~\ref{fig:scheduling_steps} would prevent workload fluctuations
to be taken into account during the computation and the application of
a schedule, potentially leading to artificial violations (\ie resource
violations that are caused by the VMPP mechanism). In other words, the
longer each phase,
% the longer the reconfiguration process,
the higher the risk that the schedule may be outdated when it is
computed or eventually applied, cf.\ the different loads during the
three phases in Figure \ref{fig:workload_fluctuations}. Similarly,
servers and network crashes can impede the detection and resolution of
resource violations if the master node crashes or if a group of VMs is
temporarily isolated from the master node.

VMPP solutions can only be reasonably evaluated if their behavior in
the presence of such adverse events can be analyzed. Providing a
framework that facilitates such studies and increases their
reproducibility is the main objective of \vmps.

\section{Simgrid, a generic toolkit}
\label{sec:sg}
%\AL[AL]{0.5page}

We now briefly introduce the toolkit on which \vmps is based.  \sg is
a toolkit for the simulation of potentially complex algorithms
executed on large-scale distributed systems.  Developed for more than
a decade, it has been used in a large number of studies described in
more than 100~publications.  Its main characteristics are the
following:
\begin{itemize}
  \item Extensibility: after Grids, HPC and P2P systems, \sg has been
    recently extended with abstractions for virtualization
    technologies (\ie Virtual Machines including a live migration
    model \cite{Hirofuchi:2013:ALM:2568486.2568524}) to allow users to
    investigate Cloud Computing challenges \cite{lucas:cloud2014}.
  \item Scalability: it is possible to simulate large-scale scenarios;
    as an example, users can simulate applications composed of
    2~million processors and an infrastructure composed of
    10,000~servers hosting more than 100,000~VMs on a computer with
    16~GB of memory. \MS[AL]{Add a ref.}
  \item Flexibility: it enables simulations to be run on arbitrary
    network topologies under dynamically changing computations and
    available network resources.
  \item Versatile APIs: users can leverage \sg through easy-to-use
    APIs for~C and~Java.
\end{itemize}

To perform simulations, users should develop a \emph{program} and
define a \emph{platform} file and a \emph{deployment} file. The
\emph{program} leverages, in most cases, the \sg MSG API that allows
end-users to create and execute \sg abstractions such as processes,
tasks, VMs and network communications. The \emph{platform} file
provides the physical description of each resource composing the
environment and on which aforementioned computations and network
interactions will be performed in the \sg world.
% (host, CPU capacity, network topology and link capacities, etc.)
The \emph{deployment} file is used to launch the different \sg
processes defined in the \emph{program} on the different nodes.
% (at least the mapping between one process and one host is mandatory
% to start the simulation)
Finally, the execution of the program is orchestrated by the \sg
engine that internally relies on an constraint solver to correctly
assign the amount of CPU/network resources to each \sg abstraction
during the entire simulation.

\sg provides many other features such as model checking, the
simulation of DAGs (Direct Acyclic Graphs) or MPI-based
applications. In the following, we only give a brief description of
the virtualization abstractions that have been recently implemented
and on which our framework relies on (for further information regarding
\sg see~\cite{casanova:hal-01017319}).

The VM support has been designed so that all operations that can be
performed on a host can also be performed inside a VM. From the point
of view of a \sg Host, a \sg VM is an ordinary task while from the
point of view of a task running inside a \sg VM, a VM is considered as
an ordinary host.
% below the task.
\sg users can thus easily switch between a virtualized and
non-virtualized infrastructure.  Moreover, thanks to MSG API
extensions, users can control VMs in the same manner as in the real
world (\eg create/destroy VMs; start/shutdown, suspend/resume and
migrate them).
% TODO: Not addressed
%\AL{Shoudl we talk about over-provisionning limitations}
For migration operations, a VM live migration model implementing the
precopy migration algorithm of Qemu/KVM has been integrated into \sg.
This model is the only one that successfully simulates the live
migration behavior by taking into account the competition arising in
the presence of resource sharing as well as the memory refreshing rate
of the VM, thus determining correctly the live migration time as well
as the resulting network
traffic~\cite{Hirofuchi:2013:ALM:2568486.2568524}.
%
%\AL[AL]{Add more details regarding live migration in order to reply to
%the Mario's remark (not clear enoug)}
%
These two capabilities are mandatory to build our VM placement
simulator toolkit.

\section{VM Placement Simulator}
\label{sec:injector}
%\AL[AL]{1.5 page}

The purpose of \vmps is to deliver a generic tool to evaluate new VM
placement algorithms and offer the possibility to compare
them. Concretely, it supports the management of VM creations, workload
fluctuations as well as node apparitions/removals.  Researchers can
thus focus on the implementation of new placement algorithms and
evaluate how they behave in the presence of changes that occur during
the simulation.
%
\vmps has been implemented in Java by leveraging the messaging API
(MSG) of \sg. Although the Java layer has an impact of the efficiency
of \sg, we believe its use is acceptable because Java offers important
benefits to researchers for the implementation of advanced scheduling
strategies, notably concerning the ease of implementation of new
strategies. As examples, we reimplemented the Snooze proposal in Java
and the DVMS proposal using Scala/Java.

In the following we give an overview of the framework and describe its
general functioning.% and how researchers can develop new algorithms

\subsection{Overview}
\label{sec:overview}

From a high-level view, \vmps performs a simulation in three phases:
(i) initialization (ii) injection and (iii) trace analysis (see Figure
\ref{fig:workflow}).  The initialization phase corresponds to the
creation of the environment, the VMs and the generation of the queue
of events that may represent, \eg load changes.  The
simulation is performed by at least two \sg processes, one executing
the \emph{injector}, which constitutes the generic part of the
framework, and a second one executing the to-be-simulated
\emph{scheduling algorithm}. During the simulation the scheduling
strategy is evaluated by injecting scheduling-relevant events.
Currently, the supported events are VM CPU load change and node
apparitions/removals that we use to simulate node crashes.  It

\begin{figure}
  {\centering ~\includegraphics[width=.95\linewidth]{figures/VMPlaceS-workflow.png}}
  \caption{\vmps's Workflow}
  \label{fig:workflow}
{\small Gray parts correspond to the generic code while the white one
  must be provided by end-users. The current version is released with
  three different schedulers (centralized/hierarchical and distributed).}
\end{figure}

% As we describe in the next section, additional events can be easily added.
%\AL[AL]{Make two figures: a architectural one (i.e. Injector vs
 % schedulers pool and one chronological.}
%\MS{Yes, the figures are important. They could also be useful to
 % partially provide a more abstract explanation.}

Users develop their scheduling algorithm by leveraging the \sg
messaging API and a more abstract interface that is provided vy \vmps
and consists of the classes \texttt{XHost}, \texttt{XVM} and
\texttt{SimulatorManager} classes. The two former classes respectively
extend \sg's \texttt{Host} and \texttt{VM} abstractions while the
latter controls the interactions between the different components of
the VM placement simulator.  Throughout these three classes, users can
inspect, at any time, the current state of the infrastructure (\ie the
load of a host/VM, the number of VMs hosted on the whole
infrastructure or on a particular host, check whether a host is
overloaded, etc.) We have used \vmps in order to analyze three
scheduling mechanisms, cf.\ Sec.~\ref{sec:vm-schedulers}, that
represent three different software architecture models: centralized,
hierarchical and fully-distributed models for VM placement.
%% TODO
%\MS{The
%  following point is too low-level and should not come here} Although
%we do not discuss that point due to space constraints, we emphasize
%that these three mechanisms enable us to deliver concrete examples of
%how the deployment file of \sg is automatically generated by leveraging
%a generic python script.  \AL{We should highlight that point in the
%  README.org}

The last phase consists in the analysis of the collected traces in
order to gather the results of the simulation, notably by means of the
generation of figures representing, \eg resource usage statistics.

%\begin{itemize}
%\item Entropy \cite{Hermenier:2009:ECM:1508293.1508300}, a centralized approach using a constraint programming approach to solve the placement/reconfiguration VM problem;
% \item Snooze \cite{feller:ccgrid12}, a hierarchical approach where
%   each manager of a group invokes Entropy to solve the
%  placement/reconfiguration VM problem. It is noteworthy that in
%   \cite{feller:ccgrid12}, Snooze is using a specific heuristic to solve the placement/reconfiguration VM problem. As the sake of simplicity, we have simply reused the entropy scheduling code.
%\item  DVMS \cite{quesnel:cpe2012}, a distributed approach that dynamically partitions the system and invokes Entropy on each partition.
% \end{itemize}

\subsection{Initialization Phase}

In the beginning, \vmps creates $n$ VMs and assigns them in a
round-robin manner to the first $p$ hosts defined in the platform
file.  The default platform file corresponds to a cluster of $h+s$
hosts, where $h$ corresponds to the number of hosting nodes and $s$ to
the number of services nodes. The values $n$, $h$ and $s$ constitute
input parameters of the simulations (specified in a Java property
file).
%% TODO
% \AL[AL]{Update the size of the cluster autonomically by
%  leveraging p + s}
These hosts are organized in form of topologies, a cluster topology
being the most common ones. It is possible, however, to define more
complex platforms to simulate, for instance, scenarios involving
federated data centers.
%Note that $s$ can be equals to 0 if the
%scheduling strategy is directly executed on the hosting nodes.

Each VM is created based on one of the predefined VM classes. A VM
class corresponds to a template specifying the VM attributes and its
memory footprint. Concretely, it is
% described as
% \texttt{nb\_cpu:ramsize:net\_bw:mig\_speed:mem\_speed}
defined in terms of five parameters: the number of cores
\texttt{nb\_cpus}, the size of the memory \texttt{ramsize}, the
network bandwidth \texttt{net\_bw}, the maximum bandwidth available
migrate it \texttt{mig\_speed} and the maximum memory update speed
\texttt{mem\_speed} when the VM is consuming 100\% of its CPU
resources. As pointed out in Section \ref{sec:sg}, the memory update
speed is a critical parameter that governs the migration time as well
as the amount of transferred data. By giving the possibility to define
VM classes, \vmps allows researchers to simulate different kinds of
workload (\ie memory-intensive vs non-intensive workloads), and thus
analyze more realistic Cloud Computing problems.  Available classes
are defined in a specific text file that can be modified according to
the user's needs.
%\MS{Follows a low-level mechanism!}
% TODO not addressed yet. This text should appear in the README
%At creation time
%of a VM, a process selects one class (\ie one line) in the file
%randomly. Hence, if a user wants to favor a specific class, he can
%simply repeat the line of the class several times.
%
Finally, all VMs start with a CPU consumption of 0 that will evolve
during the simulation in terms of the injected load as explained
below.

Once the creation and the assignment of VMs completed, \vmps spawns at
least two SG processes, the \emph{injector} and the launcher of the
selected scheduler.  The first action of the \emph{injector} consists
in creating the different event queues and merge them into a global
one that will be consumed during the second phase of the simulation.
For now, we generate two kinds of event: \emph{CPU load} and
\emph{node crash} events.
%\todo{Before apparitions/ removals}
% The former consists in changing the load of a VM by creating and
% assigning a new \sg task in the VM while the second aims at
% simulating crashes.
%
% Changing the load of a VM has a direct impact of its memory update
% speed and thus on the time to migrate it between two hosts.
The \emph{CPU load} event queue is generated in order to change the
load of each VM every $t$ seconds on average. $t$ is a random variable
that follows an exponential distribution with rate parameter
$\lambda_t$ while the CPU load of a VM evolves according to a Gaussian
distribution defined by a particular mean ($\mu$) as well as a
particular standard deviation ($\sigma$). $t$, $\mu$ and $\sigma$ are
provided as input parameters of a simulation.  As the CPU load can
fluctuate between 0 and 100\%, \vmps prevents the assignment of
nonsensical values when the Gaussian distribution returns a number
smaller than 0 or greater than 100. Although this has no impact on the
execution of the simulation, we emphasize that this can
reduce/increase the effective mean of the VM load, especially when
$\sigma$ is high.  Hence, it is important for users to specify
appropriate values.
%% TODO
%\AL[AL]{A binomial law would have solved this issue: too late too bad :(}
%Although this can have an impact on the
%effective mean, especially when $\sigma$ is high, we believe it was
%non appropriated to request it is easier for end-users to specify $\mu$ and
%$\sigma$ parameters than
Furthermore, each random process used in \vmps is initialized with a
seed that is defined in a configuration file. This way, we can ensure
that different simulations are reproducible and may be used to
establish fair comparisons.

The \emph{node crash} event queue is generated in order to turn off a
node every $f$ seconds on average for a duration of $d$ seconds.
Similarly to the $t$ value above, $f$ follows an exponential
distribution with rate $\lambda_f$. $f$ and $d$ are also provided as
input parameters of a simulation.

Adding new events can easily be done by simply defining new event Java
classes implementing the \texttt{InjectorEvent} interface and by
adding the code in charge of generating the associated queue. Such a
new queue will be merged into the global one and its events will then be
consumed similarly to other ones during the \emph{injector phase}.

\subsection{Injector Phase}

Once the VMs and the global event queue are ready, the evaluation of
the scheduling mechanism can start. First, the injector process
iteratively consumes the different events that represent, for now,
load changes of a VM or turning a node off or on. Changing the load of
a VM corresponds to the creation and the assignment of a new \sg task
in the VM. This new task has a direct impact on the time that will be
needed to migrate the VM as it increases or decreases the current CPU
load and thus% the percentage of
its memory update speed.
% \MS[AL]{Is the above paragraph clear enough?}
% that is indicated by the \texttt{mem\_speed}
%% parameter given in the class description.
When a node is turning off, the VMs that were running on that node are
temporarily discarded, \ie they are hidden and cannot be accessed
until the node comes back to life. This way, the scheduler cannot
handle them.
 %\AL[AL, MS, JP]{This is ugly but unfortunately the true,
 % it will be better to reassign those VMs on other nodes, but which
 % one?  }
We leave for future work other approaches that can better
match realistic scenarios such as turning off the VMs and
reprovisioning them on other nodes.
%

As defined by the scheduling algorithm, VMs will be suspended/resumed
or relocated on the available hosts to meet scheduling objectives and
SLA guarantees.  Note that users must implement the algorithm in
charge of solving the VMPP but also the code in charge of applying
reconfiguration plans by invoking the appropriate methods available
from the \texttt{SimulatorManager} class. This step is essential as
the reconfiguration cost is a key element of dynamic placement
systems.  \MS[AL]{maybe it is better to prevent the access to Xhost
  and XVM methods that can change the Simulator States. Hence, we
  should enforce the access only through the SimulatorManager class?
  What do you think? Yes, would be cleaner. Can we just present the
  interface as such? Or not talk about the direct possibility?}  Last
but not least, it is noteworthy that \vmps really invokes the
execution of each scheduling strategy in order to get the effective
reconfiguration plan.  That is, the computation time that is observed
is not simulated but corresponds to the effective one, only the
workload inside the VMs and the migration operations are simulated in
\sg. It is hence mandatory to propagate the reconfiguration time into
the \sg engine.%
% The following is IMO a technical detail
% by invoking a \texttt{wait} call of the MSG interface.

\subsection{Trace Analysis}
\label{subsec:traces-analysis}

The last step of \vmps consists in analyzing the information that has
been collected during the simulation.
% in order to understand and compare the behavior of the different
% algorithms.
This analysis is done in two steps. First, \vmps records several
metrics related to the platform utilization throughout the simulation
by leveraging an extended version of \sg's TRACE
module\footnote{\url{http://simgrid.gforge.inria.fr/simgrid/3.12/doc/tracing.html}}.
This way, visualization tools that have been developed by the \sg
community, such as PajeNG~\cite{pageng:www}, may be used. Furthermore,
our extension enables the creation of a trace file in the JSON file
format, which is used to generate several figures using the R
statistical environment~\cite{R:Bloomfield:2014} about the resource
usage during the simulation.

By default, \vmps records the load of the different VMs and hosts, the
appearance and the duration of each violation of VM requirements in
the system, the number of migrations, the number of times the
scheduler mechanism has been invoked and the number of times it
succeeds or fails to resolve non-viable configurations.
%
Although these pieces of information are key elements to understand
and compare the behavior of the different algorithms, we emphasize
that the TRACE API enables the creation of as many variables as
necessary, thus allowing researchers to instrument their own algorithm
with specific variables that record other pieces of information.

\section{Dynamic VMPP Algorithms}
\label{sec:vm-schedulers}
To illustrate the interest of \vmps, we implemented three dynamic VM
placement mechanisms respectively based on the Entropy
\cite{Hermenier:2009:ECM:1508293.1508300}, Snooze
\cite{feller:ccgrid12}, and DVMS \cite{quesnel:cpe2012} proposals. For the three
implementations, we chose to use the latest VMPP solver that has been
developed as part of the Entropy
framework~\cite{hermenier:cp11}.

%
Giving up consolidation
optimality in favor of scalability, this algorithm provides a ``repair
mode'' that enables the correction of VM requirement violations. The algorithm considers that a host is
overloaded when the VMs try to consume more than 100\% of the CPU
capacity of the host. In such a case, the algorithm looks for
an optimal viable configuration until it reaches a predefined timeout.
The optimal solution is a new placement that satisfies
the requirements of all VMs while minimizing the cost of the
reconfiguration.
Once the timeout has been triggered, the algorithm returns
the best solution among the ones it finds and applies the associated
reconfiguration plan by invoking live migrations in the simulation
world.

%
Although using the Entropy VMPP solver
implies a modification from the original Snooze proposal,  we
highlight that our goal is to illustrate the capabilities of \vmps and
thus we believe that such a modification is acceptable as it does not
change the global behavior of Snooze. Moreover by
conducting such a comparison, we also investigate the pros and cons of
the three  architecture models on which these proposals rely on (\ie centralized, hierarchical and
distributed).

%
Before discussing the simulation results, we
describe in this section an overview of the three implemented systems.
We highlight that the extended abstractions for hosts (\texttt{XHost})
and VMs (\texttt{XVM}) as well as the available functions of the \sg
MSG API enabled us to develop them in a direct and natural manner.


\subsection{Entropy-based Centralized Approach}
\label{subsec:entropy}
The centralized VM placement mechanism consists in one single \sg
process deployed on a service node. This process implements a simple loop that
iteratively checks the viability of the current configuration by
invoking every $p$ seconds the aforementioned VMPP solver. $p$ is
defined as an input parameter of the simulation.

% \AL{Should we explain the issue right now or not if we add VMPP section}
% Indeed, during
% the computation and the application of a schedule, the algorithm does
% not enforce QoS properties anymore, and thus cannot react quickly to
% violations. Second, since the manipulation of VMs is costly, the time
% needed to apply a new schedule is particularly important: The longer
% the reconfiguration process is, the higher is the risk that the schedule may
% be outdated, due to the workload fluctuations, when it is eventually
% applied.
% \vmps enables researchers to investigate such concerns in-depth.

As the Entropy proposal does not provide a specific mechanism for the
collect of resource usage information but simply uses an external tool
(namely ganglia), we had two different ways to implement the monitoring to
process:  either by implementing additional asynchronous transmissions
as a real implementation of the necessary state updates would proceed
or, in a much more lightweight manner, through direct accesses by the
aforementioned process to the states of the hosts and their respective
VMs. While the latter does not mimic a real implementation closely, it
can be harnessed to yield a valid simulation: overheads induced by
communication in the ``real'' implementation, for instance, can be
easily added as part of the lightweight simulation. We have
implemented this lightweight variant for the monitoring

Regarding fault tolerance, similarly to the Entropy proposal, our
implementation does not provide any failover mechanism.

Finally, as mentioned in Section \ref{subsec:traces-analysis}, we monitor, for each iteration,
whether the VMPP solver succeeds or fails. In case of success, \vmps
records the number of migration that has been performed, the time it
took to apply the reconfiguration and whether
the application of the reconfiguration plan led to new violations.

\subsection{Snooze-based Hierarchical Approach}
\label{subsec:snooze}
\input{snooze.tex}

\subsection{DVMS-based Distributed Approach}
\label{subsec:dvms}
% TODO Not adressed
%\AL[AL]{Check who write that part, If Flavien did it, then add him as
%  an author}
\input{dvms}
\section{Experiments}
\label{sec:experiments}
\AL[JP,AL,MS]{2 pages}
\AL{Il faudra parler du nombre de migrations qui est egalement une
  métrique pertinente. Plusieurs algorithms tentent de reduire cette
  metrique }
\AL[AL]{Il faudra mettre des snapshots de PajeNG}

\subsubsection{Experimental testbed}

% Evaluation of VMPlaceS on Grid'5000: simulations were running on one server.
We evaluated the behaviour of the VMPlaceS frameworks using simulations 
conducted on the Grid'5000 testbed. Each of the simulations was running on a
dedicated server, thus avoiding interferences between simulations.

% Scripts: automation of the deployment, running of simulations and the collect
% of results.

% It enables us to run a large number of simulations, with several variants
% of the scheduling algorithm.

\subsection{Comparison of Entropy, Snooze and DVMS}

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=.65\linewidth]{figures/experiments/1024-hierarchical.pdf}
    \caption{Load of the cluster.}
\end{center}
\label{fig:experiments_1024_hierarchical}
\end{figure}

% * 1024-hierarchical.pdf
% For the simulations a load of 85% has been injected.
% Stationary phase at 1200s => 1800s of simulation.
% The 3 algorithms have been tested with the same loads.

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=.65\linewidth]{figures/experiments/compute_time_per_service_node.pdf}
    \caption{Cumulated computation time per node invocking Entropy.}
\end{center}
\label{fig:experiments_compute_time_per_service_node}
\end{figure}

% * compute_time_per_service_node.pdf: 
The conductions of the simulations enables the comparison of three different 
scheduling algorithms. Firstly, even if all the implementations of these three 
algorithms are based on Entropy, their respectif amount of nodes invocking 
Entropy differs: to enable a fair comparison of the time spend on computing 
reconfiguration plan time, 
Figure \ref{fig:experiments_compute_time_per_service_node} depicts the 
computation time per node invocking Entropy. It is noticeable that the 
distributed and hierarchical algorithms are significantly below the centralised
algorithm. This can be easily explained: the centralised policy concentrates all
the computations on one node, while the distribured and the hierarchical 
policies distribute computation on several nodes.

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=.65\linewidth]{figures/experiments/violation_time.pdf}
    \caption{Cumulated violation time.}
\end{center}
\label{fig:experiments_violation_time}
\end{figure}

% * experiments_violation_time.pdf:
% - Centralized is not efficient: it violation is much more high than 
%   distributed or hierarchical.
% - Distributed approaches are better. However a flat peer to peer approach such
%   as DVMS has better results that Snooze (hierarchical).

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=.65\linewidth]{figures/experiments/migration_time.pdf}
    \caption{Cumulated migration time.}
\end{center}
\label{fig:experiments_violation_time}
\end{figure}

% * migration_time.pdf:
% - On the other hand centralised do less migrations: it can be explained by its
%   lack of reactivity.
% - Distributed and Hierarchical have similar results, even if hierarchical do
%   less migrations.

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=.65\linewidth]{figures/experiments/migration_avg_duration.pdf}
    \caption{Average migration of a migration.}
\end{center}
\label{fig:experiments_migration_time}
\end{figure}

% * migration_avg_duration.pdf:
% - Centralized was doing less migrations than the others. However, at 1024 
%   nodes, the migration time is becoming increasing dramatically: it can be
%   explained by the "first_solution" policy: to improve its reactivity entropy
%   returns the first working reconfiguration plan. At 1024 nodes, computations
%   takes too much time, thus degrading the results of entropy which programs
%   several migrations on some nodes.
%   
%


\subsubsection{Reactivity and Violation time}

\subsubsection{Fault Tolerance}
\AL[JP]{perform an experiment on Snooze and DVMS with fault
  periodicity of 10 days for instance}
\MS[JP]{perform an experiment on Snooze with additional fautls (i.e.
  reducing the fault periodicity) in order to illustrate the pros/cons
of the two different join mechanisms. To switch between one and the
other strategies, see AUX.java line 34, GLElectionForEachNewGM = false/true}
\MS[JP]{}


\subsection{Investigating Variants and Possible Improvements}

Our simulation framework facilitates the simulation of variants of
placement algorithms. In the following, we present several variants of
the placement algorithms introduced in Sec.~\ref{sec:vm-schedulers}
that have been discussed in the literature or come up during the
implementation of their models using \vmps. This section provides
strong evidence that the modification and evaluation of existing
algorithms is much facilitated by our simulation framework.



\subsubsection{Variants of hierarchical scheduling}
\label{sec:snoozeVariants}

We now present three non-trivial variants that we have implemented and
explored: periodic vs.\ reactive scheduling, a variant of the
assignment algorithm of LCs to GMs, and a variant of the algorithms of
how GMs and LCs join the system.  \MS{How and where do we provide the
  scheduling parameters for the evals.?}

\paragraph{Periodic vs.\ reactive scheduling}

Snooze~\cite{feller:ccgrid12} schedules VMs in a periodic fashion:
after a fixed time period a GM calls the scheduler in order to resolve
resource conflicts among the LCs it manages. The information whether a
resource conflict has to be handled is taken based on the summary
information that is periodically sent by the LCs to the GM.

We have implemented using \vmps an alternative, reactive, strategy to
scheduling: as soon as resource conflicts occur, LCs avert their GMs
ofthem; the GMs then immediately initiate scheduling. Implementing
this reactive scheme can be done using our framework in two
manners. First, by implementing additional asynchronous transmissions
of the necessary state updates as a real implementation would
proceed. Second, in a much more lightweight manner through direct
accesses by the GMs to the states of their respective LCs. In order to
ensure that this leightweight implementation mimics a real
implementation closely, delays induced by communication in the
``real'' implementation are accounted for (congestion issues are not
relevant in this case because a resource conflict blocks the GM and
its LCs anyway). We have implemented this lightweight variant of
reactive scheduling including explicit modeling of communication
delays. Using the abstractions provided by \vmps, in particular
harnessing its extended notion of hosts that represent the VMs managed
by the LCs of a VM, reactive scheduling has been implemented by adding
4~lines of code.

\begin{table*}[ht]
\begin{center}
\begin{tabular}{|P{30mm}||c|c|c|c||c|c|c|c|}
    \thickhline
    \textbf{Algorithm}
      & \multicolumn{4}{c||}{\textbf{No.\ migrations}}
      & \multicolumn{4}{c|}{\textbf{Total violation time (s)}}
        \Tstrut \\
    ~\hfill{\small \#LCs/\#GMs}  & 64/6 & 128/12 & 256/25 & 512/51 &  64/6 & 128/12 & 256/25 & 512/51 \Bstrut \\
    \thickhline
      Reactive      & 30 & 89 & 179 & 338 & 267 & 764 & 1638 & 3088 \\
      Periodic 120s & 9 & 24 & 69 & 142 & 814 & 2548 & 6238 & 16038
    \Rstrut \\ \hline
    \thickhline
\end{tabular}
\end{center}
    \caption{Reconfiguration nos. and violation times for reactive and
    periodic scheduling}
\label{tbl:reactiveRes}
\end{table*}

We have shown the usefulness of our framework by exploring some
properties of reactive scheduling compared to periodic scheduling. To
this end we have simulated reactive scheduling and a periodic
algorithm that reconfigures all LCs every 2 mins for configurations
ranging from 64 to 512 LCs. These simulations have yielded, among
others, the results shown in Tbl.~\ref{tbl:reactiveRes}. The results
clearly show that while a reactive strategy entails a much higher
number of migrations (because the periodic one misses many overload
situations), reactive scheduling results in a significantly lower
total migration time.


\paragraph{Assignment of LCs to GMs}

LCs are assigned to GMs by the GL as part of the LC join protocol. In
Snooze's native implementation LCs are assigned in a round-robin
fashion to the known GMs. If GMs join (and leave) the system at the
same time as LCs, a round-robin (RR) strategy at join time, however,
does not ensure an even distribution. This may happen, for instance at
startup time of the system, when new GMs and LCs enter the system, or
in case of failures, which trigger GM and LC joins. In order to
evaluate the corresponding imbalance and its consequences we have
implemented the LC assignment protocol in a modular fashion and
applied it to different highly-dynamic settings in which GMs and LCs
enter the system at the same time. Furthermore, we have implemented a
best-fit (BF) strategy that assigns LCs to the GMs with minimal load
or, if several GMs with minimal load exist, to the GMs with the
smallest number of assigned LCs.


% \subsubsection{LC assignment in Snooze-like placement alg.}
% \label{sec:snoozeVariantsEval}

% \begin{figure}[ht]
% \begin{center}
%     \includegraphics[width=.95\linewidth]{figures/violationTime-snooze-RR-BF.pdf}
%     \caption{Cumulated violation time for BF (lower line) and RR
%       (upper line) variants}
% \end{center}
% \label{fig:snoozeBFRRViolation}
% \end{figure}

\begin{table*}[ht]
\begin{center}
%    \begin{tabular}{|c!{\vrule width 3pt}c|c|c!{\vrule width 2pt}c|c|c!{\vrule width 3pt}c|c!{\vrule width 2pt}c|c|}
    \begin{tabular}{|P{30mm}|||c|c||c|c|||M{30mm}|M{30mm}|}
        \thickhline
        \multirow{3}*{\textbf{Strategy}}
          & \multicolumn{4}{c|||}{\textbf{\#LCs/\#GMs}}
          & \multicolumn{2}{c|}{\textbf{Total violation time (s)}}
          \Tstrut \\
          & \multicolumn{2}{c||}{128 LCs, 12 GMs}
            & \multicolumn{2}{c|||}{256 LCs, 25 GMs}
          & \multirow{2}*{128 LCs, 12 GMs} & \multirow{2}*{256 LCs, 25 GMs}
          \Bstrut \\
          & range & stdev & range & stdev & &  \Bstrut \\
        \thickhline
        Best-Fit & 0--30 & 10.53 & 0--18 & 6.62  & 395 & 1005 \Rstrut \\
        Round-Robin & 0--49 & 15.7  & 0--35 & 12.22 & 630 & 1265
        \Rstrut \\ \hline
        \thickhline
    \end{tabular}
\end{center}
    \caption{LCs to GM assignment and cumulated violation times for RR
      and BF strategies}
    \label{tbl:assignmentResults}
\end{table*}


We have evaluated the two LC assignment strategies using \vmps on
configurations of~128 and~256 nodes. In order to clearly expose the
corresponding differences this evaluation has been performed by
``stressing'' the two strategies by simultaneously
starting all LCs (128/256) and all GMs (12/25) and then simulating the
resulting configuration over a one hour period.  These experiments
have yielded the following results, cf.\ Table~\ref{tbl:assignmentResults}:
\begin{itemize}
  \item BF yields more homogeneous assignments of LCs to GMs: the
    ranges of the numbers of LCs assigned to a GM and their standard
    deviations are significantly smaller.\footnote{Here, some GMs may
      be assigned 0 LCs if some GMs join the system after all LCs have
      been assigned.}
  \item The cumulated time spent resolving violations is, for both
    configurations, significantly smaller for BF than for RR.
\end{itemize}
From these results, we can clearly infer that BF is significantly
better than RR for the two tested kinds of configuration. Furthermore,
we conjecture that BF should perform at least as good as RR for all
configurations (the proof is left to future work).
% \MS[JP]{I need the exact figures for the violation times
%   in Tbl.~\ref{tbl:assignmenResults}}

\paragraph{Variants of the join algorithms}

The join algorithms, see Sec.~\ref{sec:snoozeAlgs}, are crucial to
Snooze for two main reasons: (i) they have to be efficient because
they can easily form a bottleneck if large numbers of LCs (GMs) have
to be registered at a GM (LC); (ii) they are multi-phase protocols
whose correctness especially in the presence of faults is difficult to
ensure.

In order to investigate the corresponding trade-offs, we have used our
framework to implement join algorithms that may be interrupted at any
time, repeat the the on-going phase a number of times before
reinitiating, if necessary, the entire protocol. Furthermore, the join
protocol is parameterized, \eg, in the number of threads used to
handle registration requests.

Finally, our framework has enabled us to test another aspect of
Snooze's join algorithm as presented by
Feller~\etal.~\cite{feller:ccgrid12}, a strategy we call the GM rejoin
strategy (GRJ): all GMs and the LCs assigned to them should rejoin if
a new GM enters the system. While GRJ supports a form of load
balancing (because all LCs are reassigned to the new set of GMs), our
simulation has shown that this strategy significantly increases the
time necessary for registering GMs and LCs compared to a simpler
strategy that does not modify existing GMs in case a new GM enters the
system. This handicap is particularly pronounced if joins of GMs may
be interrupted due to faults. Concretely, experiments involving 20 GMs
and 200 LCs have shown that this strategy often multiplies the time
necessary to join all 220 components by 10 or more compared to the
simple join strategy. While the qualitative result that the more
complex strategy presented in the paper results in a more
time-consuming join process is not very surprising, the extent of the
resulting degradation was surprising.



\subsubsection{DVMS Analysis}
\AL{if space and time add PajeGN view for DVMS}

\section{Related Work}
\label{sec:related}
\AL[AL]{.25 page}

Several simulator toolkits have been proposed since the last years in
order to adress CC concerns~\cite{CC13, DGSIM, cloudsim, icancloud,
  greencloud}.  They can be classified into two categories: The first
one corresponds to ad-hoc simulators that have been developped to
address a particular concern. For instance, CReST~\cite{CC13} is a
discrete event simulation toolkit built to evaluate Cloud provisioning
algorithms. If ad-hoc simulators enable to provide some trends
regarding the bevahiours of the system, they do consider the
implication of the different layers, which can lead to non
representative results at the end. Moreover, such ad-hoc solutions are
developped for one shot and thus, they are not available for the
scientific community. The second category \cite{icancloud, greencloud,
  cloudsim} corresponds to more generic cloud simulator toolkits (\ie
they have been designed to adress a majority of CC
challenges). However, they have focused mainly on the API and not on
the model of the different mechanisms of CC systems.

For instance, CloudSim~\cite{cloudsim}, which has been widely used to
validate algorithms and applications in different scientific
publications, is based on a relatively top-down viewpoint of cloud
environments.  That is, there is no papers that properly validate the
different models it relies on: a migration time is calculated by
dividing a VM memory size by a network bandwidth.
%Such a model cannot correctly simulate many real
%environments where workloads perform substantial memory writes.
 In addition to having inaccuracy weaknesses at the low level, available cloud
simulator toolkits over simplified the model for the virtualization
technologies, leading also to non representation results at the
end. As highlighted several times throughout this document, we chose to
build \vmps on top of \sg in order to benefit fromt its accuracy of
its models related to virtualization abstractions~\cite{Hirofuchi:2013:ALM:2568486.2568524}.

\section{Conclusion}
\label{sec:conclusion}

In this paper we have presented \vmps, a framework providing
programming support for the definition of VM placement algorithms,
execution support for their simulation at large scales, as well as new
means for their trace-based analysis. \vmps enables, in particular,
the investigation of placement algorithms in the context of numerous
and diverse real-world scenarios. We have validated it by evaluating
and comparing algorithms representative for three different classes of
virtualization environments: centralized, hierarchical and fully
distributed placement algorithms. We have also shown how \vmps
facilitates the implementation and evaluation of variants of placement
algorithms. The corresponding results have provided the first
systematic results comparing these algorithms in environments
including thousands of nodes and tens of thousands virtual machines.

Future work : (i) network changes and other dimensions (I/O), (ii) VM
provisioning (i.e.  Adding/removing VMs in the system,


% conference papers do not normally have an appendix


\section{Acknowledgment}

This work is supported by the French ANR project SONGS (11-INFRA-13).
Experiments have been performed using the Grid'5000 experimental
testbed, being developed under the INRIA ALADDIN development action
with support from CNRS, RENATER and several Universities as well as
other funding bodies (see https://www.grid5000.fr).


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
