
%%%% TODO: Adrien, this subsection has been already addressed in the previous subsection, so I commented it
%\subsection{Implementation of DVMS}
%
%A prototype of DVMS leveraging the \emph{peer actor} abstraction has been 
%developed. In addition we built two versions of the network overlay actor: one 
%working with Chord, and one working with the Vivaldi based overlay. This mean 
%that now DVMS is network overlay agnostic to, and thus can be used with either
%of the network overlay without requiring any modification in it's source code.
%
%The implementation is based on modern programming language and framework such as 
%\emph{Scala} and \emph{Akka framework}. Scala is a language that mixes object
%oriented programming with functional programming, it's compiler produces 
%\emph{Java bytecode} which can be run in any JVM environment. Combining Scala 
%with Akka enabled us to take advantage of advanced techniques for concurrent 
%programming such as \emph{future/promise} and \emph{actor model}, and to benefit
%from Java ease of deployment.
%


%\subsection{Grid'5000 experiments}

%\JP{Complete this section with more experimentation results.}

%\subsubsection{Objectives}
%The prototype has been tested with a various number of experiments conducted on
%the Grid'5000 testbed. 
The main objective of the experiments we conducted was to estimate
impact of locality on the performance of a distributed scheduling algorithm. 
A significant portion of the reconfiguration time is spent in live migration of
virtual machines, which depends of network parameters such as latency and
bandwidth. One way to improve performance of distributed scheduling algorithm is
is to promote collaboration between close ressources, which can be reach by 
maximising this ratio: $nb\ intrasite\ migrations/nb\ migrations$.
%\[
%	\frac{number\ of\ intrasite\ migrations}{number\ of\ migrations}
%\]


\subsection{Experimental protocol}
To compare our experiments, we implemented a dedicated injector that makes load
changes of VMs during a predefined time.  VMs are launched on PMs in a
round-robin manner, \ie each PM has almost the same number of VMs at the
beginning, The experiment consists in  repeatedly changes target CPU loads of
VMs.  Every $t$ second, the injector that is deployed on a dedicated node,
selects one VM and changes its CPU load according to a Gaussian distribution.
$t$ is a random variable that follows an exponential distribution with rate
parameter $\lambda$. The Gaussian distribution is defined by a mean ($\mu$) as
well as a standard deviation ($\sigma$) that are given at the beginning of the
experiment. 
exponential and the Gaussian distributions previously described. 
The parameters are $\lambda=\mathit{Nb\_VMs}/300$ and $\mu=70$, $\sigma=30$.
Concretely, the load of each VM starts from 0\% and varies on average every 5
min in steps of 10 (with a significant part between 40\% and 100\% of CPU
usage).   Each experiment duration was set to 3600 seconds.

For each experiment, we booked 40 compute servers spread on 4 geographical sites (10 servers per site)
and 1 service server from the Grid'5000 testbed. The compute servers were used 
to run virtual machines and DVMS while the service node runs the aforementioned
injector.
%
Each compute node was equiped with 8 cores and was hosting a number of virtual
machines proportional to its number of CPU cores
 ($nb VM\ =\ 1.3\ \times\ nb\ cores)$, leading to an global number of 416 VMs. 
Although such a number is rather small regarding the latest experiments that have been performed on DVMS \cite{quesnel:ispa2013}, our goal
is not to validate once again the scalability criteria but to focus on the locality aspect of such an algorithm. 
%\[
%	number\ of\ virtual\ machines\ =\ 1.3\ \times\ number\ of\ cores
%\]

\subsection{Results}

\subsubsection{Maximization of intra-site migrations}

Table~\ref{migration_table} compares ratio between intra-site migrations and the
total number of migrations, using Chord or Vivaldi. The results show that the impact of locality is
significant: using a Vivaldi based network overlay leads to an average number of
86.3\% of intra-site migrations while using a Chord based DVMS decreases this
ratio to 49.6\%.

\begin{table}

  \begin{center}
    \begin{tabular}{|c|c|c|}   

      % <HEADER>
      \hline \multicolumn{1}{|p{3cm}|}{ }
       & \multicolumn{1}{|p{3cm}|}{\centering Chord }  & \multicolumn{1}{|p{3cm}|}{ \centering Vivaldi}  \\
      % </HEADER>

      % <ROW 1> => Average
      \hline
      average & 0.496 & 0.863 \\
      % </ROW 1>

      % <ROW 2> => Min
      \hline
      minimum & 0.378 & 0.798 \\
      % </ROW 2>

      % <ROW 2> => Max
      \hline
      maximum & 0.629 & 0.935 \\
      % </ROW 2>

      \hline
    \end{tabular}
  \end{center}
  \caption{\label{migration_table} Comparison of intra-site migrations ratio between Chord and Vivaldi overlay.}
\end{table}

\subsubsection{Dynamic clustering}

During our investigation of the Vivaldi based results, we noticed that many of
the inter-site migrations were performed between Luxembourg and Nancy. In 
table~\ref{latency_table} we measure the latency that exists between every site
involved in our experiments. It is noticeable that Luxembourg and Nancy have a
low latency, which is quite normal considering that Nancy is separated by only
100 kilometers.


\begin{table}

  \begin{center}
    \begin{tabular}{|c|c|c|c|c|}   

      % <HEADER>
      \hline \multicolumn{1}{|p{2cm}|}{ } & \multicolumn{1}{|p{2cm}|}{\centering Grenoble }  & \multicolumn{1}{|p{2cm}|}{\centering Luxembourg } & \multicolumn{1}{|p{2cm}|}{\centering Nancy }& \multicolumn{1}{|p{2cm}|}{\centering Rennes } \\
      % </HEADER>

      % <ROW 1> => Grenoble
      \hline
      Grenoble & 0.09 ms & 16.55 ms & 14.24 ms & 15.92 ms \\
      % </ROW 1>

      % <ROW 2> => Luxembourg
      \hline
      Luxembourg &  & 0.17 ms & 2.70 ms & 13.82 ms \\
      % </ROW 2>

      % <ROW 3> => Nancy
      \hline
      Nancy & &  & 0.27 ms & 11.42 ms \\
      % </ROW 3>

      % <ROW 4> => Rennes
      \hline
      Rennes &  &  &  & 0.23 ms \\
      % </ROW 3>

      \hline
    \end{tabular}
  \end{center}
  \caption{\label{latency_table} Latency measured between sites.}
\end{table}

This means that Vivaldi enabled DVMS to learn which site is more interesting to
perform VM migration : promoting low latency inter-site collaboration have made 
many of inter-site migrations acceptable compared to those executed by the Chord
version.


\subsubsection{Reactivity}


\begin{table}

  \begin{center}
    \begin{tabular}{|c|c|c|}   

      % <HEADER>
      \hline \multicolumn{1}{|p{3cm}|}{ }
       & \multicolumn{1}{|p{3cm}|}{\centering Chord }  & \multicolumn{1}{|p{3cm}|}{ \centering Vivaldi}  \\
      % </HEADER>

      % <ROW 1> => Size
      \hline
      average size (servers) & 3.918 & 2.337 \\
      % </ROW 1>

      % <ROW 2> => Number of sites
      \hline
      average number of sites involved & 1.645 & 1.082 \\
      % </ROW 2>

      % <ROW 2> => Duration
      \hline
      average duration (msec) & 154.63 & 98.50 \\
      % </ROW 2>

      \hline
    \end{tabular}
  \end{center}
  \caption{\label{microcosm_table} Comparison of microcosms metrics between Chord and Vivaldi overlay.}
\end{table}


Table~\ref{microcosm_table} depicts metrics that allow for an objective 
comparison of the efficiency of both network overlay. Firstly, using the Vivaldi
based overlay decrease the number of servers that are involved in microcosms by
41\%, meaning that collaboration between close nodes has become more efficient.

Secondly, it is interesting that using the Vivaldi overlay leads to a 46\% lower
microcosm duration than encountered with Chord. This result is consistent with 
the fact that with Vivaldi, the number of sites that are involved in microcosms
becomes very close to one site : collaborating with closer servers enable
scheduling algorithm to minimize latency between peers, thus considerably 
increasing the reactivity of the system.
% 
