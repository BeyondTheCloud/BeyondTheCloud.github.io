
%%%% TODO: Adrien, this subsection has been already addressed in the previous subsection, so I commented it
%\subsection{Implementation of DVMS}
%
%A prototype of DVMS leveraging the \emph{peer actor} abstraction has been 
%developed. In addition we built two versions of the network overlay actor: one 
%working with Chord, and one working with the Vivaldi based overlay. This mean 
%that now DVMS is network overlay agnostic to, and thus can be used with either
%of the network overlay without requiring any modification in it's source code.
%
%The implementation is based on modern programming language and framework such as 
%\emph{Scala} and \emph{Akka framework}. Scala is a language that mixes object
%oriented programming with functional programming, it's compiler produces 
%\emph{Java bytecode} which can be run in any JVM environment. Combining Scala 
%with Akka enabled us to take advantage of advanced techniques for concurrent 
%programming such as \emph{future/promise} and \emph{actor model}, and to benefit
%from Java ease of deployment.
%


%\subsection{Grid'5000 experiments}

%\JP{Complete this section with more experimentation results.}

%\subsubsection{Objectives}
%The prototype has been tested with a various number of experiments conducted on
%the Grid'5000 testbed. 
The main objective of the experiments we conducted was to estimate impact of locality on
the performance of a distributed scheduling algorithm. A significant portion of the
reconfiguration time is spent in live migration of virtual machines, which depends of
network parameters such as latency and bandwidth. One way to improve performance of
distributed scheduling algorithms is to promote collaboration between close ressources,
which can be reached by maximising this ratio: $nb\ intrasite\ migrations/nb\ migrations$.
%\[
%	\frac{number\ of\ intrasite\ migrations}{number\ of\ migrations}
%\]


\subsection{Experimental Protocol}

To compare our experiments, we implemented a dedicated injector that makes load changes of
VMs during a predefined time. VMs are launched on PMs in a round-robin manner, \ie each
PM has roughly the same number of VMs at the beginning, The experiment consists in
repeatedly changes target CPU loads of VMs. Every $t$ seconds, the injector that is
deployed on a dedicated node selects one VM and changes its CPU load according to a
Gaussian distribution. $t$ is a random variable that follows an exponential distribution
with rate parameter $\lambda$. The Gaussian distribution is defined by a mean ($\mu$) as
well as a standard deviation ($\sigma$) that are given at the beginning of the experiment.
%exponential and the Gaussian distributions previously described. 
The parameters are $\lambda=\mathit{Nb\_VMs}/300$ and $\mu=70$, $\sigma=30$.
Concretely, the load of each VM starts from 0\% and varies on average every 5
min in steps of 10 (with a significant part between 40\% and 100\% of CPU
usage). Each experiment duration was set to 3600 seconds.

For each experiment, we booked 40 compute servers spread over 4 geographical sites (10
servers per site) and 1 service server from the Grid'5000 testbed. The compute servers
were used to run virtual machines and DVMS while the service node runs the aforementioned
injector.
%
Each compute node was equiped with 8 cores and was hosting a number of virtual machines
proportional to its number of CPU cores ($nb VM\ =\ 1.3\ \times\ nb\ cores)$, leading to
an global number of 416 VMs. Although such a number is rather small regarding the latest
experiments that have been performed on DVMS~\cite{quesnel:ispa2013}, our goal is not to
validate once again the scalability criteria but to focus on the locality aspect of such
an algorithm.
%\[
%	number\ of\ virtual\ machines\ =\ 1.3\ \times\ number\ of\ cores
%\]

\subsection{Results}

\subsubsection{Maximization of Intra-Site Migrations}

Table~\ref{migration_table} compares the ratio between intra-site migrations and the total
number of migrations, using Chord or our locality-based overlay (LBO). The results show that the impact of locality
is significant: using LBO leads to an average number of 86.3\%
of intra-site migrations while using a Chord based DVMS decreases this ratio to 49.6\%.

\begin{table}

  \begin{center}
    \begin{tabular}{|c|c|c|}   

      % <HEADER>
      \hline \multicolumn{1}{|p{3cm}|}{ }
       & \multicolumn{1}{|p{3cm}|}{\centering Chord }  & \multicolumn{1}{|p{3cm}|}{ \centering LBO}  \\
      % </HEADER>

      % <ROW 1> => Average
      \hline
      average & 0.496 & 0.863 \\
      % </ROW 1>

      % <ROW 2> => Min
      \hline
      minimum & 0.378 & 0.798 \\
      % </ROW 2>

      % <ROW 2> => Max
      \hline
      maximum & 0.629 & 0.935 \\
      % </ROW 2>

      \hline
    \end{tabular}
  \end{center}
  \caption{\label{migration_table} Comparison of intra-site migrations ratio
    between Chord and our locality-based overlay.}
\end{table}

\subsubsection{Dynamic Clustering}

During our investigation of the results brought about LBO, we noticed that many of the
inter-site migrations were performed between Luxembourg and Nancy sites. In
Table~\ref{latency_table}, we present the latency that exists between every site involved
in our experiments. It is noticeable that Luxembourg and Nancy have a low latency, which
is quite normal considering that Nancy is separated by only 100 kilometers.
\\
This means that LBO enabled DVMS to learn which site is more interesting to perform VM
migration. Promoting low latency inter-site collaboration have made many of inter-site
migrations acceptable compared to those executed by the Chord version.


\begin{table}[t!]

  \begin{center}
    \begin{tabular}{|c|c|c|c|c|}   

      % <HEADER>
      \hline \multicolumn{1}{|p{2cm}|}{ } & \multicolumn{1}{|p{2cm}|}{\centering Grenoble }  & \multicolumn{1}{|p{2cm}|}{\centering Luxembourg } & \multicolumn{1}{|p{2cm}|}{\centering Nancy }& \multicolumn{1}{|p{2cm}|}{\centering Rennes } \\
      % </HEADER>

      % <ROW 1> => Grenoble
      \hline
      Grenoble & 0.09 ms & 16.55 ms & 14.24 ms & 15.92 ms \\
      % </ROW 1>

      % <ROW 2> => Luxembourg
      \hline
      Luxembourg &  & 0.17 ms & 2.70 ms & 13.82 ms \\
      % </ROW 2>

      % <ROW 3> => Nancy
      \hline
      Nancy & &  & 0.27 ms & 11.42 ms \\
      % </ROW 3>

      % <ROW 4> => Rennes
      \hline
      Rennes &  &  &  & 0.23 ms \\
      % </ROW 3>

      \hline
    \end{tabular}
  \end{center}
  \caption{\label{latency_table} Latency measured between sites.}
\end{table}


% \vspace{-1.0cm}


% \vspace{-1.3cm}

\subsubsection{Reactivity}
\begin{table}[t!]

  \begin{center}
    \begin{tabular}{|c|c|c|}   

      % <HEADER>
      \hline \multicolumn{1}{|p{3cm}|}{ }
       & \multicolumn{1}{|p{3cm}|}{\centering Chord }  & \multicolumn{1}{|p{3cm}|}{ \centering LBO}  \\
      % </HEADER>

      % <ROW 1> => Size
      \hline
      average size (servers) & 3.918 & 2.337 \\
      % </ROW 1>

      % <ROW 2> => Number of sites
      \hline
      average number of sites involved & 1.645 & 1.082 \\
      % </ROW 2>

      % <ROW 2> => Duration
      \hline
      average duration (msec) & 154.63 & 98.50 \\
      % </ROW 2>

      \hline
    \end{tabular}
  \end{center}
  \caption{\label{microcosm_table} Comparison of microcosms metrics between
    Chord and our locality-based overlay.}
\end{table}

% \vspace{-1.0cm}

Table~\ref{microcosm_table} depicts metrics that allow for an objective comparison of the
efficiency of both network overlays. Firstly, using the LBO decreases the
number of servers that are involved in microcosms by 41\%, meaning that collaboration
between close nodes has become more efficient.

Secondly, it is interesting that using the LBO leads to a 46\% lower
microcosm duration than encountered with Chord. This result is consistent with 
the fact that with our locality-aware overlay, the number of sites that are involved in microcosms
becomes very close to one: collaborating with closer nodes enable
to perform scheduling/reconfiguring phases much faster since migration operations are
shorter in time, thus increasing considerably the reactivity of the system.
%% TODO (ici ca aurait ete bien de donner un odre de grandeur sur la difference entre le temps de migration intra-site vs inter-sites pour deux VMS de meme classe (i.e. qui ont la meme memory intensity, celle d√©fini dans le code de l'injector). 

% 
