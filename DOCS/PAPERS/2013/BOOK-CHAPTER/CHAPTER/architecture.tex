%%%
\section{Premises of a LUC OS: The \discovery Proposal\label{sec:archi}}

In this section, we propose to go one step further by discussing preliminary
investigations around the design and the implementation of a first LUC OS
proposal: the \discovery system~(DIStributed and COoperative framework to manage
Virtual EnviRonments autonomouslY). We draw the premises of the \discovery
system by emphasizing some of the challenges as well as some research directions
to solve them. Finally, we give some details regarding the prototype that is
under development and how we are going to evaluate it.  

\subsection{Overview}

The \discovery system relies on a multi-agent peer-to-peer system deployed on
each physical resource composing the LUC infrastructure. Agents are autonomous
entities that collaborate with one another to efficiently use the LUC resources. In our context,
efficiency means that a good trade-off is found between users'
expectations, reliability, reactivity and availability,
while limiting the energy consumption of the system and providing
scalability. 
%To reduce the management complexity as well as the design and the
%implementation of the critical mechanisms, we strongly
%support the use of micro-kernel concepts. Such an approach should enable us to design
%and implement services at higher level while leveraging peer-to-peer mechanisms
%at the lower ones.
%Furthermore, to address the different objectives and reduce
%the management complexity, we also underline that self-* properties should be
%present at every level of the system.  We think that relying on a multi-agent
%peer-to-peer system is the best solution to cope with the scale as well as the
%network disconnections that may create temporary partitions in a LUC platform.

In \discovery, each agent has two purposes: (i)~maintaining a knowledge base on the
composition of the LUC platform and (ii)~ensuring the correct execution of VEs. 
%Concretely, the knowledge base will consist of overlays that will be used 
%for the autonomous management of the
%VEs life cycle.
This includes the configuration, deployment and monitoring of
VEs as well as the dynamic allocation or relocation of VMs to adapt to changes
in VEs requirements and physical resources availability. To this end, agents
will rely on dedicated mechanisms related to: 

\begin{itemize}
\item The localization and monitoring of physical resources, 
\item The management of VEs, 
\item The management of VM images, 
\item Reliability,
\item Security and privacy.
\end{itemize}

\subsection{Resource Localization and Monitoring Mechanisms\label{ssec:p2p}}

Keeping in mind that \discovery should be designed in a fully decentralized
fashion, its mechanisms should be built on top of an overlay network able to
abstract out changes that occur at the physical level. The specific requirements
of this platform will lead to the development of a novel kind of overlay
networks, based on locality and a minimalistic design.
%
More concretely, the first step is to design, at the lowest level, an overlay
layer intended to hide the details of the physical routes and computing
utilities, while satisfying some basic requirements such as locality and
availability. This overlay needs to enable the communications between any two
nodes in the platform. While overlay computing has been extensively studied over
the last decade, we emphasize here on minimalism, and especially on one key
feature to implement a LUC OS:  retrieving nodes that are geographically close
to a given departure node.

\subsubsection*{Giving Nodes a Position}

The initial configuration of the physical network can take an arbitrary
shape. We choose to rely on the Vivaldi
protocol~\cite{dabek:2001:sigcomm04}. Vivaldi is a distributed algorithm
assigning coordinates in the plane to nodes of a distributed system. Each node
is equipped with a \emph{view} of the network, \emph{i.e.}, a set of nodes it
knows. This view is initially assumed as random. Coordinates obtained by a node
reflects its \emph{position} in the network, \emph{i.e.}, close nodes in the
network are given close coordinates in the plane. To achieve this, each node
periodically checks the round trip time between itself and another node
(randomly chosen among nodes in its view) and adapts its distance (by changing
its coordinates) with this node in the plane accordingly. See
Figure~\ref{fig:vivaldi_before} and Figure~\ref{fig:vivaldi_after} for an
illustration of 4 nodes~(A, B, C and D) moving according to the Vivaldi
protocol. 
%
A globally accurate positioning of nodes can be
obtained if nodes have a few long-distance nodes in their
view~\cite{dabek:2001:sigcomm04}. These long distance links can be easily
maintained by means of a simple gossip protocol.

\begin{figure}[!b]
	\vspace*{-.3cm}
  \begin{minipage}[c]{.45\linewidth}
   \hspace*{-0.5cm}
      	\centering \includegraphics[width=3.4cm]{./FIGS/vivaldi_before.pdf}

   \hspace*{0.5cm}
		\caption{Vivaldi plot before updating positions. Each node pings other nodes. Each node maintains a map of distance.}
\label{fig:vivaldi_before}
   \end{minipage}
\hspace*{0.6cm}
   \begin{minipage}[c]{.45\linewidth}
   	\centering \includegraphics[width=3.4cm]{./FIGS/vivaldi_after.pdf}
		\caption{Vivaldi plot after updating positions. The computed
                  positions of other nodes have been updated.}
		\label{fig:vivaldi_after} 
  \end{minipage} \hfill
\end{figure}


\subsubsection*{Searching for Close Nodes}

Once the map is achieved (each node knows its coordinates), we are able to decide
whether two nodes are \emph{close} by calculating their distance. However, the view of
each node does not \emph{a priori} contain its closest nodes. Therefore, we need additional
mechanisms to locate a set of nodes that are close to a given initial node --
Vivaldi gives a \emph{location} to each node, but not a neighborhood. To achieve
this, we use a modified distributed version of the classic Dijkstra's algorithms
used to find the shortest path between two nodes in a graph. The goal is to
build a \emph{spiral}\footnote{The term \emph{spiral} is here a misuse of
language, since the graph actually drawn in the plane
might contain crossing edges. The only guarantee is that when following the
path constructed, the nodes are always further from the initial node.}
interconnecting the nodes in the plane that are the closest ones to a given initial
node.

Let us consider that our initial point is a node called $I$. The first step is
to find a node to build a two-node spiral with $I$. Such a node is sought in the
view of $I$ by selecting the node, say $S$, having the smallest distance with
$I$. $I$ then sends its view to $S$, $I$ stores $S$ as its successor in the
spiral, and $S$ adds $I$ as its predecessor in the spiral. Then $I$ forwards its
view to $S$. $S$ creates a new view by keeping the $n$ nodes which are the
closest to $I$ in the views of $I$ and $S$. This last view is then referred to
as the \emph{spiral view} and is intended to contain a set of nodes among which
to find the next step of the spiral. Then $S$ restarts the same process: Among
the spiral view, it chooses the node with the smallest distance to $I$, say
$S'$, and adds it in the spiral -- $S$ becomes the predecessor of $S'$ and $S'$
becomes the successor of $S$. Then, the spiral view is sent to $S'$ which
updates it with the nodes it has in its own view. The process is repeated until
we consider that enough nodes have been gathered (a parameter sent by the
application).

Note that one risk is to be blocked by having a spiral view containing only
nodes that are already in the spiral, leading to the impossibility to build the
spiral further. However, this problem can be easily addressed by forcing the
presence of a few long distance nodes whenever it is updated.

\subsubsection*{Learning}

Applying the protocol described above, the quality of the spiral is
questionable in the sense that the nodes that are actually close to the starting
node $s$ may not be included. The only property ensured is that one step
forward on the built path always takes us further from the initial node.

To improve the \emph{quality} of the spiral, \emph{i.e.}, reduce the average
distance between the nodes it comprises and the initial node, we add a learning
mechanism coming with no extra communication cost: when a node is contacted for
becoming the next node in one spiral, and receives the associated spiral view,
it can also keep the nodes that are the closest to itself, thus potentially
increasing the quality of a future spiral construction.

\subsubsection*{Routing}

In the context of a LUC infrastructure, one crucial feature is to be able to
locate an existing VM. Having the same strategy consisting in improving the
performance of the overlay based on the activity of the application, we envision a
routing mechanism which will be improved by past routing requests. By means of the
spiral mechanism, a node is able to contact its neighboring
nodes to start routing a message.

This initial routing mechanism can be very expensive, as the number of hops can
be linear in the size of the network. However, from previous communications, a
node is able to memorize long links to different locations of the
network. Consequently, from each routing request, the source of the request and
each node on the path to the destination are able to learn long links, which
will significantly reduce the number of hops of future requests. We are
currently studying the amount of requests needed to get close to a logarithmic
routing complexity. More generally, we are working on the estimation if the
activity of the application required to (i)~guarantee the constant efficiency of
the overlay and to (ii)~converge, starting from a random configuration, to a
fully-efficient overlay network.

%\ftodo[CT]{TODO}

% Each node doing so in parallel, a number of clusters are created inside which we
% can ensure a certain level of locality -- all nodes inside this group can
% communicate with each other very efficiently. Given the number of nodes in these
% groups, the inner topology of a group can either rely on very simple graphs,
% such as rings, or more connected graphs, to accelerate the dissemination and
% retrieval of information in the group. Note that, still based on simple
% gossiping techniques, such graphs can be easily maintained as the network's
% conditions change. For instance, if a link becomes overloaded, the other nodes
% will react to this change by removing nodes with which they communicate through
% this link from their local group. Such groups are exemplified on
% Figure~\ref{fig:renater_overlay} (for the west part of the platform).

% \begin{figure}[htbp]
% \includegraphics[width=12cm]{./FIGS/renater_overlay.png}
% \caption{Overlay local groups on top of the RENATER platform.\label{fig:renater_overlay}}
% \end{figure}

% As we can see on Figure~\ref{fig:renater_overlay}, it follows from the way the
% overlay is built that some nodes may be a member of several groups. The Nantes
% site for instance is part of three of these groups, given its
% physical position. More generally, the overlay will take the shape of a set of
% groups with some \emph{bridges} between the groups. Note that several
% \emph{bridges} can interconnect two local groups. Thus, any request first goes
% through local nodes allowing for its local processing, and avoiding the need for
% global coordination mechanisms.

% To be able to localize a particular VM, the system has to be able to route a
% request from any node to any other node. This functionality is the classic
% problem of P2P overlays, which is traditionally solved in structured overlay networks by
% maintaining a routing table on each node, and by a flooding mechanism or with
% random walks in unstructured overlays. These techniques are usually designed for
% very large scale networks with different guarantees and costs. Here, given the
% particular ``intermediate'' scale of the platform, and its specific
% requirements, we believe that these existing techniques are too
% \emph{powerful}. As we want to design a minimalistic overlay, there is room to
% try to design a new routing technique specifically fitting our
% requirements. The aim is then to maintain, in all groups, information about the
% distance between this group and \emph{close} groups in terms of number of bridge nodes
% to go through. Hence, the system will be able to route quickly between \emph{close}
% groups. The routing of requests between \emph{far} groups will be based on a random
% decision when no information are available, but \emph{oriented} by the aim of
% going away from the request's source.

% This overlay will provide the basic building block of the platform, on which will
% rely higher level overlays and functionalities, which are described in the
% following sections.

% \subsubsection*{Upper-Layer overlays}

% \ftodo[CT]{To be completed...}


% \ftodo[AL]{To be completed by Cedric and Marin}
% \ftodo[AL]{C/P from the proposal : 
% self-organizing overlay construction mechanisms based on gossip and epidemic
% techniques that will link the \discovery resources in navigable graphs onto
% which requests for VE can be routed and allocated. It will investigate
% mechanisms to quickly adapt these graphs to changing conditions and
% infrastructure, and mechanisms to monitor the adequacy of previous requests
% solutions to these changing conditions as these occur.}

\subsection{VEs Management Mechanisms}
\label{ssec:vem}
In the \discovery system, we define a VE as a set of VMs that may have specific
requirements in terms of hardware, software and also in terms of placement:~
For instance, some VMs must be on the same node/site to cope with performance objectives while
others should not be collocated to ensure
high-availability criteria~\cite{hermenier:2013}.
As operations on a VE may occur in any place from any location, each agent should provide the capability
to configure and start a VE, to suspend/resume/stop it, to relocate some of its VMs if need be or simply to retrieve the location of a particular VE. 
Most of these mechanisms are provided by current UC platforms. However, as mentioned before, they
should be revisited to correctly run on the infrastructure we target
(\textit{i.e.} in terms of scalability, resiliency and reliability).
To this aim, the \discovery system relies on the aforementioned peer-to-peer mechanisms. 

As a first example, placing the VMs of a VE requires to be able to find available nodes that
fulfill the VM needs (in terms of resource requirements as well as placement
constraints). Such a placement can start locally, close to the client
application requesting it, \textit{i.e.}, in its local group. If no such node is
found, a simple navigation ensures that the request will encounter a bridge,
leading to the exploration of further nodes. This navigation goes
on until an adequate node is found.
A similar process is performed by the mechanism in charge of  dynamically
controlling and adapting the placement of VEs during their lifetime.  For instance, 
to ensure the particular needs of a VM, it can be necessary to relocate other VMs.
According to the predefined constraints of VEs, some VMs might be
relocated on far nodes while others would prefer to be suspended.  Such a
mechanism has been deeply studied and validated in the DVMS
framework~\cite{dvms:wiki,quesnel:2012}. DVMS~(Distributed Virtual
Machine Scheduler) is able to dynamically schedule a significant number of VMs
throughout a large-scale distributed infrastructure while guaranteeing VM
resource expectations.  

A second example regards the networking configuration of VEs.
Although it might look simple, assigning the right IP to
each VM as well as maintaining the intra-connectivity of a VE becomes a bit more complex than in
the case of a single network domain, \textit{i.e.} a single site deployment.
%
Keeping in mind that a LUC infrastructure is
by definition spread WANwide, a VE can be hosted between distinct network
domains during its lifetime. No solution has been chosen yet. 
Our first investigations led us to leverage techniques
such as the IP over P2P project~\cite{ganguly:2006}. However,
software-defined networking becomes more and more important; investigating proposals such as
the Open vSwitch project~\cite{pfaff:2009} looks promising to solve such an issue.
%

\subsection{VM Images Management}

In a LUC infrastructure, VM images could be deployed in any place from any
other location. However, being in a decentralized, large-scale, heterogeneous and widely spread
environment makes the management of VM images more difficult than with
conventional centralized repositories.  
At coarse grain, the management of the VM images should be (i)~consistent
with regard to the location of each VM in the \discovery infrastructure and
(ii)~reachable in case of failures.
%
The envisioned mechanisms to manage VM images have been
classified into two categories.
%
First, some mechanisms are required to efficiently
upload VM images and replicate them across many nodes, to ensure
efficiency as well as reliability.  Second, other mechanisms 
are needed to schedule VM image
transfers. Advanced policies are important to improve the efficiency of each
transfer that may occur either during the first deployment of a VM or during its relocations. 

Regarding storage and replication mechanisms,  an analysis of an IBM Cloud concludes
that a fully distributed approach using peer-to-peer technology is not the best choice to manage VM images, since the
number of instances of the same VM image is rather small~\cite{peng:2012}. However, central or
hierarchical solutions are not suited for the infrastructure we target.
Consequently, an improved peer-to-peer solution working with replicas and
deduplication has to be investigated, to provide more
reliability, speed, and scalability to the system. For example, analyzing
different VM images shows that at least 30\% of the image is shared between
different VMs. This 30\% can become a 30\% reduction in space, or a
30\% increase in reliability or in transfer speed. Depending on the
situation, we should decide to go from one scenario to another. 
%Finally, the number of replicas and deduplicated data should be dynamically balanced.

Regarding the scheduling mechanisms, a study showed that VM boot time can increase
from 10 to 240 seconds when multiple VMs running I/O intensive tasks use the
same storage system~\cite{tan:2008}. Some actions, like providing the
image chunks needed to boot first~\cite{tang:2011}, defining a new
image format, and pausing the rest of the I/O operations, can provide a
performance boost and limit the overhead that is still observed in commercial
Clouds~\cite{mao:2012}. 
%These actions should also take into account
%power-like metrics in order to reduce the energy consumption of data transfers.
%According to (Preist et Shabajee Nov 2010) the cost to transmit 1 MB can be of
%4Wh. Considering the size of VM images, any improvement aiming at reducing
%data movement will make a big difference.

More generally, the amount of data linked with VM images is significant.
Actions involving data  should be aware of consequences on metrics like
(but not limited to): energy efficiency, reliability, proximity, bandwidth and
hardware usage. The scheduler could also anticipate actions, for instance moving images when
the load is low or the energy is cheap.

%% \subsection{Reliability Mechanisms}
%% %These mechanisms should ensure  reliability and high availability of the
%% %\discovery system despite the scale and dynamicity of the underlying physical
%% %infrastructure.   
%% By nature, a LUC is a highly distributed platform where 
%% node and network failures will be much more frequent than in
%% actual UC platforms.  Furthermore, since resources could be located anywhere,
%% the expected mean time to repair failed equipments might be much larger than in
%% other platforms. For all these reasons, a set of dedicated mechanisms should be designed in order
%% to provide a fully transparent failure management with minimum downtime. 
%% %
%% To this aim, the \discovery system should include, first,  mechanisms in charge of its own reliability. 
%% Such mechanisms are required to avoid losing or corrupting important information
%% regarding the state of the system.  Of course, handling all kinds of failures
%% and implementing a fully resilient operating system is a complex task. Hence,
%% we propose to consider in a first time a crash-stop failure model. In other words, the
%% \discovery system should be able to autonomously restart any service by
%% relocating it on an healthy agent each time it is mandatory.  This implies to
%% define a common design pattern, \textit{i.e.} a set of recommendations, that each
%% service should follow to ensure such characteristics. Besides, in order to
%% ensure that such a pattern may be applied to stateful services, the system can
%% require a Cassandra like system \cite{lakshman:2010} that provides reliable and highly available
%% storage for critical system states. Retrieving the information related to one
%% service need to rely on the kind of mechanisms described in Section~\ref{ssec:p2p}.

%% %
%% In addition to be robust enough, the \discovery system should ensure the
%% reliable execution of virtual environments. The first mechanisms consists in
%% using snapshotting capabilities delivering by virtualization technologies.
%% Concretely, each internal states of a VM should be periodically saved on a
%% persistent storage.  When a crash occurs on a VM, its associated VE can be
%% restarted from the latest consistent states, \textit{i.e.} all VMs of the VE will be
%% resume for their latest snapshot.  As for the other mechanisms, performing VM
%% snapshotting in a large-scale, heterogeneous and widely spread environment is a
%% challenging task. However, we believe that aforementioned mechanisms in charge
%% of the VM images as well as recent proposals \cite{nicolae:2011} might enable
%% to provide such a feature. 
%% Although VM snapshotting provides a first level of reliability, it is not
%% sufficient to ensure high availability of the VE. More advanced mechanisms must
%% be proposed.  Our idea is to include mechanisms based on primary-backup
%% replication techniques. 
%% The basic principle is to have one active replica of the VM (the primary)
%% sending state updates to the other replicas (the backups) periodically. If the
%% primary fails, one of the backup can resume the execution transparently for the
%% outside world. Furthermore since the entire VM is replicated, applications can
%% be run unmodified.  Solutions to replicate VMs inside a cluster have been
%% proposed. However efficiently replicating VMs over a WAN is a huge challenge.
%% Limiting the size of the backup updates\cite{rajagopalan:2012}, and
%% reducing the impact of the required synchronizations on the execution of the
%% primary \cite{gerofi:2012}  are research directions to be further
%% studied. A better understanding of the parts of a VM that really need to be
%% updated is required. It might require to trade transparency for performance by
%% allowing latency-sensitive applications to define which part of their state has
%% to be updated.

%% \ftodo[AL]{Integrate the P2P aspect into the reliability section}
%% \subsubsection*{Monitoring Example}

%% Another key use of this low-level overlay is proactive replication of VMs,
%% keeping in mind that two identical VMs should be placed in relatively distant
%% nodes, for fault-tolerance reasons (close nodes have a high probability to fail
%% together). Following the defined overlay structure, this can be done through a
%% navigating scheme where at least one bridge is encountered. Monitoring this
%% replica can be done easily by having a \emph{watcher} in the same local group as
%% the replica.

\input{reliability}

\subsection{Security and Privacy Mechanisms}
%
%LUC OS should include a mechanism for the security of the lower layers and particularly multiple P2P overlays.
%% Previous research~\cite{sybilattacks} has shown that without trusted identities, it 
%% is not possible to protect a P2P overlay against sybil attacks. 
%Recent advances~\cite{Castro:2002:SRS:844128.844156} in DHT 
%security might enable to provide a secured overlay. 
%But they are not sufficient to ensure the good behavior of resources and users.
%LUC OS should include an authentication and certification mechanism that 
%evaluates the behavior of resources and VEs.
%% Finally, as the resources could be hosted in more or less secured locations and provided by different 
%% resource providers, this certification should also be used to rank the security of each resources.
%% \ftodo[AL->JRC]{Could you please put only the most relevant one}
%
%Moreover, LUC OS shoud integrate security decision and enforcement points (SDEPs).
%%  mechanisms at all locations and layers.
%To guarantee a security, 
%Sandhu~\cite{sandhu_towards_2010} propose a roadmap towards such security mechanisms for
%Cloud where the Cloud infrastructure provides security hooks and
%mechanisms. 
%% To realize this goal, they point out the need for
%% \emph{developing models, methodologies and architectures for
%%   decentralized dynamic management of security and assurance
%%   policies}. 
%Similarly than other mechanisms, 
%As LUC infrastructure is even more distributed, heterogeneous and 
%dynamic than Cloud, its SDEPs should be distributed and 
%decentralized and able to evolve according to security requirements 
%without central security decision points. Futhermore, 
%LUC OS should provide a protocol to ease the collaboration between the SDEPs.
%Indeed, when new resources join
%a LUC infrastructure, SDEPs from different locations 
%must collaborate to extend the LUC infrastructure while keeping it secured. This collaboration between 
%SDEPs will also append when a VE is spread over different 
%resources and/or migrate from one to another.
%
%LUC OS should allow users to express their security requirements on their VEs.
%The expression of these requirements itself is a complex task.
%% LUC OS must propose to the user a way to model their VEs and 
%% the security requirements on them. 
%To ease the expression of these 
%requirements, LUC OS must propose a domain specific security language 
%that defines high-level security requirements such as \cite{rouzaud_book13b,Bacon:2010:EEA:2023718.2023739} do 
%for Clouds. These security requirements will be enforced 
%by SDEPs during the whole life time of the VE.
%
%%%%
% 1./ Securiser les overlays
% 3.a./ Definir  un DSL pour les VEs
% 3.b/ Offrir des moyens d'integrer des SDEPs (i.e. moulinette qui maintient/execute les DSLs)
% 2./ Securiser l'acces aux operations de manipulation de l'infra (admin et user)

% 1
To be successful, \discovery needs to provide mechanisms and methods to construct trust relationships between resource providers.
Trust relationships are known to be complex to build~\cite{Miller:2010:TWT:1907636.1907726}. Providing strong authentication, assurance and certification mechanisms for 
providers and users is required but it is definitely not enough. Trust covers socio-economic aspects that must be addressed but are out of the scope of this chapter.
The challenge is to provide a trusted \discovery base.

% 2 
As overlays are fundamentals for all \discovery mechanisms, another challenge is to ensure
that they are not compromised. Recent advances~\cite{Castro:2002:SRS:844128.844156} might enable to tackle such concerns.

%3.A ? 
The third challenge will consist in (i)~providing end-users with a way to define their own security and privacy policies and (ii)~ensuring that these policies are enforced. 
The expression of these policies itself is a complex task, since it requires
to improve the current trade-off between security (and privacy) and usability.
To ease the expression of these policies, we are currently designing a domain specific language
to define high-level security and privacy requirements~\cite{rouzaud_book13b,alefray:hpdc:2013}. 
These policies will be enforced in a decentralized manner,
by distributed security and privacy decision and enforcement points~(SPDEPs) during the lifetime of the VEs.
Implementing such SPDEP mechanisms in a distributed fashion will require to
conduct specific research, since currently there are only prospective proposals for
classic UC infrastructures~\cite{Bacon:2010:EEA:2023718.2023739,sandhu_towards_2010}. 
Therefore, we need to investigate whether such proposals can be adapted to the LUC
infrastructure by leveraging appropriate overlays.
%%  

% Finally, as traditional distributed systems, \discovery should ensure the good behavior of resources and users.
% To this aim, authentication and certification mechanisms should be provided. 

\subsection{Toward a First Proof of Concept}

The first prototype is under heavy development. It aims at delivering a
simple mock-up for integration/collaboration purposes.  Following the
coarse-grained architecture described in the previous sections, we have
started to identify all the components participating in the system, their
relationships, as well as the resulting interfaces. 
Conducting such a work now is mandatory to move towards a more complete as well
as more complex system. 

To ensure a scalable and reliable design, we  chose to rely on the use
of high-level programming abstractions; more precisely, we are using distributed
complex event programming~\cite{janiesch:2011} in association with the actors
model~\cite{agha:1986}. This enables us to easily switch between a push and a pull
oriented approach depending on our needs. 
%To make the integration of the mechanisms easier, we propose to follow a Service
%Oriented Architecture \cite{valipour:iccsit09}.

Our preliminary studies showed that a common building block is mandatory to
handle resiliency concerns in all components. Concretely, it corresponds to a
mechanism in charge of throwing notifications that are triggered by the low
level network overlay each time a node joins or leaves it.  Such a mechanism
makes the design and the development of higher building blocks easier as they do
not have to provide specific portions of code to monitor infrastructure
changes. 

This building block has been designed around the \emph{Peer Actor} concept (see Figure~\ref{fig:supervisor} and
Figure~\ref{fig:peeractor}).
 The \emph{Peer Actor} serves as an interface between higher services
and the communication layer. It provides methods that enable to define the behaviors of a
service when a resource joins or leaves a particular peer-to-peer overlay as well as
when neighbors change.
Considering that several overlays may co-exist in the \discovery system, 
the association between a \emph{Peer Actor} and its \emph{Overlay Actor} is
done at runtime and can be changed on the fly if need be. However, it is noteworthy that 
each \emph{Peer Actor} takes part to one and only one overlay at the same time.  
%
In addition to the \emph{Overlay Actor}, a \emph{Peer Actor}  is composed of a
\emph{Notification Actor}  that processes events and notifies registered actors.
%
As illustrated in Figure~\ref{fig:peeractor}, a service can use more than one \emph{Peer Actor} (and reciprocally). 
Mutualizing a \emph{Peer Actor} enables for instance to reduce the network overhead implied by the maintenance of the overlays. 
In the example, the first service relies on a \emph{Peer Actor} implementing a Chord
overlay, while the second service uses an additional \emph{Peer Actor} implementing a CAN structure.
 
\begin{figure}
  \begin{minipage}[c]{.35\linewidth}
	\vspace*{.5cm}
   \hspace*{-0.5cm}
      	\centering \includegraphics[width=2.8cm]{./FIGS/PeerActor.pdf}
   \hspace*{0.5cm}
	\vspace*{.5cm}
		\caption{The \emph{Peer Actor} Model. The \emph{Supervisor Actor} monitors all the actors it encapsulates while the \emph{Peer Actor} acts as an interface between the services and the overlay.}
\label{fig:supervisor}
   \end{minipage}
\hspace*{0.6cm}
   \begin{minipage}[c]{.55\linewidth}
   	\centering \includegraphics[width=4cm]{./FIGS/PeerActorServices.pdf}
	\vspace*{-0.25cm}
		\caption{A \emph{Peer Actor} Instantiation. The first service relies on a \emph{Peer Actor} implementing a Chord
overlay while the second service uses an additional \emph{PeerActor} implementing a CAN structure.}
		\label{fig:peeractor} 
  \end{minipage} \hfill
	\vspace*{-0.4cm}
\end{figure}

By such a mean, higher-level services can take the advantage of the advanced
communication layers without dealing with the burden of managing the different
overlays. As an example, when a node disappears, all services that
have been registered as dependent on such an event are notified. 
Service actors can thus react accordingly to the behavior that has been specified. 
%
%Finally, to ensure the reliable execution of the different actors, each one is supervised by a parent entitled 
%the {Supervisor Actor} (see Figure \ref{fig:SupervisorActor}).
%
%
% we consider that at this scale, failures are the norm
%rather than the exception, so we decided that each actor will be monitored
%by a \emph{Supervisor actor}. \discovery services are under the supervision of the
%\
%
%: an actor may crash for a variety of reasons
% we decided that each of the system's actors will be
%supervised by a parent actor called \emph{Supervisor actor} (figure 
%\ref{fig:SupervisorActor}): an actor may crash for a variety of reasons
%(network disruption, byzantine failure, etc.) and it is normal to consider that
%different kind of failures can lead to different reactions of the system. These 
%reactions will be decided by the \emph{Supervisor actor}: reboot of the actor,
%escalation of the error, etc.

Regarding the design and the implementation of the \discovery system, each
service is executed inside its own actor and communicates by
exchanging messages with the other ones. This ensures that each
service is isolated from the others: When a service crashes and needs to be
restarted, the execution of other services is not affected. 
% Besides, each service will be supervised by a global actor that encapsulates all
% services, ie. the \discovery agent. Each time a service fails, the \discovery
% agent catches and analyzes the failure so that it can decide which operations
% to perform. 
%
As previously mentioned, we consider that at the LUC infrastructure scale, failures are the norm
rather than the exception; hence we decided that each actor would be monitored
by a \emph{Supervisor Actor} (see Figure~\ref{fig:supervisor}). \discovery services are under the supervision of the \discovery agent: This design allows to precisely define a strategy
that will be executed in case of service failures. This will be the way to
introduce self-healing and self-organizing properties to the \discovery system.

This building block has been fully implemented\footnote{Code is available at:
\href{https://github.com/BeyondTheClouds}{\url{https://github.com/BeyondTheClouds}}} by 
leveraging the Akka/Scala framework~\cite{akka:www}.

As a proof of concept, we are implementing a first high level service in charge
of dynamically scheduling VMs across a LUC infrastructure by leveraging the
DVMS~\cite{quesnel:2012} proposal (see Section~\ref{ssec:vem}). The low-level
overlay that is being currently implemented is a robust ring based on the  Chord  algorithm
combined with the Vivaldi positioning system: It enables services to select nodes that have
low latency, so that collaboration will be more efficient. 

% When a node cannot guarantee the QoS for its
%hosted VMs, it starts an iterative scheduling procedure (ISP) by querying its
%neighbor to find a better placement. If the request cannot be satisfied by the
%neighbor (i.e., there is no viable placement to ensure the QoS of all VMs
%hosted on both reserved nodes), the request is forwarded to the following free
%node that takes part in that ISP.  The `absorption' of a new node is repeated
%until the ISP succeeds.  This approach allows DVMS to consider only a minimal
%number of nodes, thus decreasing the scheduling time without requiring a
%central coordinator.  Moreover, it allows several ISPs to occur independently
%at the same moment throughout the infrastructure.  To prevent deadlocks that
%may occur when all nodes are reserved by active ISPs, a distributed deadlock
%prevention mechanism has been designed. It enables DVMS to merge pairs of
%partitions.  To sum things up, the DVMS proposal has been designed to be fully
%distributed, non-predictive and event-driven by using partial views of the
%system.  Such a mechanism perfectly fits the requirements of a LUC OS.  The
%missing part was to extend DVMS in order to take into account resiliency
%aspects. This is going to be solved as we are implementing the DVMS algorithm
%with the \emph{Peer Actor}. 
%

%To validate the behavior, the performance as well as the reliability of our
%POC, we rely first on the Simgrid~\cite{Casanova:2008:SGF:1397760.1398183}
%toolkit. Simgrid has been recently extended to integrate virtualization abstractions and accurate migration models. 
%Simulations enable us to analyze particular situations and get several metrics
%that cannot be easily monitored  on a real platform.
%Second, results obtained from simulations are then compared to real experiments on the Grid'5000 platform. 
%Grid'5000 provides a testbed supporting experiments on various types of
%distributed systems (high-performance computing, grids, peer-to-peer systems,
%Cloud Computing, and others), on all layers of the software stack. The core
%testbed currently comprises 10 sites geographically spread across France. 
%For the Discovery purpose, we developed a set of scripts that enables to deploy in a \emph{one-click} fashion a large number of VMs throughout 
%the whole infrastructure\cite{flauncher}. 
%
To validate the behavior, the performance as well as the reliability of our
proof of concept~(POC), we are performing several experiments on the Grid'5000 testbed\footnote{\url{https://www.grid5000.fr}}
that comprises hundreds of nodes distributed on 10 computing sites that are geographically spread across France. 
To make experiments with \discovery easier, we developed a set of scripts that can deploy thousands of VMs throughout 
the whole infrastructure in a \emph{one-click} fashion~\cite{flauncher}.  
By deploying our POC on each node and by
leveraging the VM deployment scripts, we can evaluate real scenario by injecting specific workloads in the different VMs. 
The validation of this first POC is almost completed. 
The resulting system will be the first to provide reactive,
reliable and scalable
reconfiguration mechanisms of virtual machines in a fully distributed and
autonomous way. This new result will pave the way for a complete proposal of
the \discovery system. 



